{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "Learning rate set to 0.001\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Vasundhara Gupta\n",
    "Raluca Niti\n",
    "\n",
    "Referenced from http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "'''\n",
    "\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "ROOT_TRAINING_DIR = 'segregated_train'\n",
    "ROOT_TEST_DIR = 'rand_segregated_test'\n",
    "ORIGINAL_COMBINED_TEST_DIR = 'X_Test'\n",
    "\n",
    "DEFAULT_NUM_EPOCHS = 5\n",
    "\n",
    "#the next two functions are used to transform images into tensors\n",
    "def training_data_transform():\n",
    "    '''Data augmentation and normalization'''\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),  # needs to be 224 pixels at minimum,\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),  # analogous to numpy ndarray\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "def eval_data_transform():\n",
    "    '''Just normalization with no augmentation'''\n",
    "    return transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),  # needs to be 224 pixels at minimum\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "def training_dataset_from_dir(root_dir):\n",
    "    return datasets.ImageFolder(root_dir, training_data_transform())\n",
    "\n",
    "\n",
    "def eval_dataset_from_dir(root_dir):\n",
    "    return datasets.ImageFolder(root_dir, eval_data_transform())\n",
    "\n",
    "\n",
    "def loader_from_dataset(dataset):\n",
    "    return torch.utils.data.DataLoader(dataset,\n",
    "                                       batch_size=4,  # 4\n",
    "                                       shuffle=True,  # True\n",
    "                                       num_workers=4)  # 4\n",
    "\n",
    "\n",
    "def train_model(model, dataset_loader, dataset_size, criterion, optim_scheduler, num_epochs=DEFAULT_NUM_EPOCHS):\n",
    "    '''\n",
    "    Following code referenced from http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "    '''\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "\n",
    "        optimizer = optim_scheduler(model, epoch)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        dataset = dataset_loader.dataset\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i, data in enumerate(dataset_loader):\n",
    "            # get the inputs and wrap in Variable\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients to avoid accumulating them when backpropagating\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # at this point outputs is a a 4x2 FloatTensor\n",
    "            # in its second return, torch.max will return the position of the\n",
    "            # max element in each row (as a LongTensor i.e. an integer)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backpropagate to compute gradient descent\n",
    "            loss.backward()\n",
    "            # update weights using function defined in optim_scheduler_ft function\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.data[0]\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            epoch_acc = running_corrects / dataset_size\n",
    "\n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format('train', epoch_loss, epoch_acc))\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print('****')\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    return best_model\n",
    "\n",
    "def test_model(model, dataset, flattened_tensor_to_image_filename):\n",
    "    dataset_loader = loader_from_dataset(dataset)\n",
    "\n",
    "    csvfile = open('output2.csv', 'w')\n",
    "    csv_writer = csv.writer(csvfile, delimiter=',')\n",
    "\n",
    "    # Iterate over data.\n",
    "    for i, data in enumerate(dataset_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # at this point outputs is a a 4x2 FloatTensor\n",
    "        # in its second return, torch.max will return the position of the\n",
    "        # max element in each row (as a LongTensor i.e. an integer)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for i, d in enumerate(inputs.cpu().data):\n",
    "            prediction = preds[i]\n",
    "            image_filename =  flattened_tensor_to_image_filename[tuple(d.numpy().flatten())]\n",
    "            pred = preds[i][0]\n",
    "\n",
    "            csv_writer.writerow([image_filename, str(pred)])\n",
    "\n",
    "# function used to determine the weight adjustments in the backward phase of training\n",
    "def optim_scheduler_ft(model, epoch, init_lr=0.001, lr_decay_epoch=7):\n",
    "    '''\n",
    "    Learning rate scheduler\n",
    "    Referenced from http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "    '''\n",
    "    lr = init_lr * (0.1 ** (epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('Learning rate set to {}'.format(lr))\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def pretrained_resnet_model():\n",
    "    # 34-layer model\n",
    "    model = models.resnet34(pretrained=True)  # pretrained on imagenet\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "    return model\n",
    "\n",
    "test_dataset = eval_dataset_from_dir(ROOT_TEST_DIR)\n",
    "\n",
    "# Hacky approach: create map from pixel data to image filename, as there's\n",
    "# no convenient way to get the original image filename from the output of\n",
    "# the model\n",
    "flattened_tensor_to_image_filename = {}\n",
    "for image_filename, _ in test_dataset.imgs:\n",
    "    img = datasets.folder.default_loader(image_filename)\n",
    "    img = eval_data_transform()(img)\n",
    "    img = tuple(img.numpy().flatten())\n",
    "    flattened_tensor_to_image_filename[img] = os.path.basename(image_filename)\n",
    "\n",
    "training_dataset = training_dataset_from_dir(ROOT_TRAINING_DIR)\n",
    "training_dataset_loader = loader_from_dataset(training_dataset)\n",
    "\n",
    "model = pretrained_resnet_model()\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    dataset_loader=training_dataset_loader,\n",
    "    dataset_size=len(training_dataset),\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optim_scheduler=optim_scheduler_ft,\n",
    "    num_epochs=5,  # DEFAULT_NUM_EPOCHS\n",
    ")\n",
    "\n",
    "test_model(model, dataset=test_dataset, flattened_tensor_to_image_filename=flattened_tensor_to_image_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
